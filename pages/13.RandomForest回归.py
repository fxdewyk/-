import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'preprocess_params' not in st.session_state:
    st.session_state.preprocess_params = {
        'scale_numeric': True,
        'encode_categorical': True,
        'missing_strategy': 'åˆ é™¤å«ç¼ºå¤±è¡Œ'
    }

st.set_page_config(page_title="ğŸŒ² é«˜çº§éšæœºæ£®æ—å›å½’", layout="wide")
st.title("ğŸŒ² é«˜çº§éšæœºæ£®æ—å›å½’åˆ†æ")

# ==================== æ•°æ®é¢„å¤„ç†æ¨¡å— ====================
def data_preprocessing(df):
    with st.expander("ğŸ”§ æ•°æ®é¢„å¤„ç†è®¾ç½®", expanded=True):
        col1, col2 = st.columns(2)

        # ç¼ºå¤±å€¼å¤„ç†
        with col1:
            st.markdown("### ğŸš« ç¼ºå¤±å€¼å¤„ç†")
            st.session_state.preprocess_params['missing_strategy'] = st.selectbox(
                "å¤„ç†æ–¹å¼",
                ["åˆ é™¤å«ç¼ºå¤±è¡Œ", "æ•°å€¼åˆ—å¡«å……å‡å€¼", "åˆ†ç±»åˆ—å¡«å……ä¼—æ•°"],
                key="missing_strategy"
            )

        # ç‰¹å¾å·¥ç¨‹
        with col2:
            st.markdown("### ğŸ›  ç‰¹å¾å·¥ç¨‹")
            st.session_state.preprocess_params['scale_numeric'] = st.checkbox(
                "æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾",
                value=True,
                key="scale_numeric"
            )
            st.session_state.preprocess_params['encode_categorical'] = st.checkbox(
                "ç¼–ç åˆ†ç±»ç‰¹å¾",
                value=True,
                key="encode_categorical"
            )

    # åº”ç”¨ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
    if st.session_state.preprocess_params['missing_strategy'] == "åˆ é™¤å«ç¼ºå¤±è¡Œ":
        return df.dropna()
    return df

# ==================== ä¸»ç¨‹åºæµç¨‹ ====================
uploaded_file = st.sidebar.file_uploader("ğŸ“‚ ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    processed_df = data_preprocessing(df.copy())

    # ==================== ç‰¹å¾é€‰æ‹©ç•Œé¢ ====================
    with st.sidebar:
        st.markdown("## ğŸ¯ ç›®æ ‡å˜é‡è®¾ç½®")
        target_col = st.selectbox("é€‰æ‹©ç›®æ ‡åˆ—", processed_df.columns)

        st.markdown("## ğŸ“Š ç‰¹å¾é€‰æ‹©")
        feature_cols = st.multiselect(
            "é€‰æ‹©é¢„æµ‹ç‰¹å¾",
            [col for col in processed_df.columns if col != target_col]
        )

        st.markdown("## âš™ é«˜çº§å‚æ•°è®¾ç½®")
        with st.expander("æ ‘å‚æ•°è®¾ç½®"):
            params = {
                'n_estimators': st.slider("æ ‘çš„æ•°é‡", 10, 1000, 100),
                'max_depth': st.slider("æœ€å¤§æ·±åº¦", 1, 50, 5),
                'min_samples_split': st.slider("æœ€å°åˆ†å‰²æ ·æœ¬", 2, 20, 2)
            }

        with st.expander("äº¤å‰éªŒè¯è®¾ç½®"):
            cv_settings = {
                'cv_folds': st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 2, 10, 5),
                'test_size': st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)
            }

    if feature_cols:
        # ==================== æ•°æ®é¢„å¤„ç†ç®¡é“ ====================
        numeric_features = processed_df[feature_cols].select_dtypes(include=np.number).columns.tolist()
        categorical_features = list(set(feature_cols) - set(numeric_features))

        # æ•°å€¼å‹ç‰¹å¾å¤„ç†
        numeric_steps = [('imputer', SimpleImputer(strategy='mean'))]
        if st.session_state.preprocess_params['scale_numeric']:
            numeric_steps.append(('scaler', StandardScaler()))

        numeric_transformer = Pipeline(numeric_steps)

        # åˆ†ç±»å‹ç‰¹å¾å¤„ç†
        categorical_steps = [('imputer', SimpleImputer(strategy='most_frequent'))]
        if st.session_state.preprocess_params['encode_categorical']:
            categorical_steps.append(('encoder', OneHotEncoder(handle_unknown='ignore')))

        categorical_transformer = Pipeline(categorical_steps)

        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ])

        # ==================== æ¨¡å‹è®­ç»ƒ ====================
        X = processed_df[feature_cols]
        y = processed_df[target_col]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=cv_settings['test_size'],
            random_state=42
        )

        # è¶…å‚æ•°ç½‘æ ¼æœç´¢
        model = GridSearchCV(
            RandomForestRegressor(),
            param_grid={k: [v] for k, v in params.items()},
            cv=cv_settings['cv_folds'],
            scoring='neg_mean_squared_error'
        )
        model.fit(preprocessor.fit_transform(X_train), y_train)

        # ==================== æ¨¡å‹è¯„ä¼° ====================
        st.subheader("ğŸ“ˆ æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        y_pred = model.predict(preprocessor.transform(X_test))

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("RMSE", f"{np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
        with col2:
            st.metric("RÂ² Score", f"{r2_score(y_test, y_pred):.4f}")
        with col3:
            st.metric("æœ€ä½³å‚æ•°", str(model.best_params_))

        # ==================== å¯è§†åŒ–æ¨¡å— ====================
        tabs = st.tabs(["ğŸ“ˆ é¢„æµ‹æ•ˆæœ", "â­ ç‰¹å¾é‡è¦æ€§", "ğŸ” SHAPè§£é‡Š", "ğŸ“Š æ®‹å·®åˆ†æ"])

        with tabs[0]:
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.scatter(y_test, y_pred, alpha=0.6)
            ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
            ax.set_xlabel("å®é™…å€¼")
            ax.set_ylabel("é¢„æµ‹å€¼")
            st.pyplot(fig)

        with tabs[1]:
            importance = model.best_estimator_.feature_importances_
            features = preprocessor.get_feature_names_out()

            fig, ax = plt.subplots(figsize=(10, 6))
            pd.Series(importance, index=features).nlargest(10).plot.barh(ax=ax)
            ax.set_title("Top 10 é‡è¦ç‰¹å¾")
            st.pyplot(fig)

        with tabs[2]:
            explainer = shap.TreeExplainer(model.best_estimator_)
            shap_values = explainer.shap_values(preprocessor.transform(X_test))

            fig, ax = plt.subplots(figsize=(10, 6))
            shap.summary_plot(shap_values, preprocessor.transform(X_test),
                            feature_names=features, plot_type="bar")
            st.pyplot(fig)

        with tabs[3]:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            residuals = y_test - y_pred
            ax1.hist(residuals, bins=30)
            ax1.set_title("æ®‹å·®åˆ†å¸ƒ")

            ax2.scatter(y_pred, residuals, alpha=0.6)
            ax2.axhline(0, color='red', linestyle='--')
            ax2.set_title("æ®‹å·® vs é¢„æµ‹å€¼")

            st.pyplot(fig)

        # ==================== æ¨¡å‹ä¿å­˜æ¨¡å— ====================
        st.sidebar.markdown("## ğŸ’¾ æ¨¡å‹ç®¡ç†")
        if st.sidebar.button("ä¿å­˜å½“å‰æ¨¡å‹"):
            joblib.dump({
                'model': model.best_estimator_,
                'preprocessor': preprocessor,
                'feature_cols': feature_cols
            }, 'random_forest_model.pkl')
            st.sidebar.success("æ¨¡å‹å·²ä¿å­˜ä¸º random_forest_model.pkl")

else:
    st.info("ğŸ“¥ è¯·ä¸Šä¼ CSVæ ¼å¼çš„æ•°æ®æ–‡ä»¶")
